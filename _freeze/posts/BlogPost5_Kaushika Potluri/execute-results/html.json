{
  "hash": "b92bd8b1080024c13889795feb41e044",
  "result": {
    "markdown": "---\ntitle: \"Sentiment Analysis on Covid-19 Vaccine\"\neditor: visual\ndesription: \"Sentiment Analysis and Topic Modeling on Covid-19 Vaccine\"\ndate: \"10/30/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - Final Project \n  - Kaushika Potluri\n---\n\n\n# Introduction\nOver just the past two years, the COVID-19 outbreak has quickly become a global emergency. While the coronavirus is severely contagious and deadly, people respond to the Covid-19 vaccine with mixed feelings on social media.\nThis project is interested in whether and how people change their attitudes towards the COVID-19 vaccine. Specifically, the project would like to answer the following research questions:\nIn this project, I am going to predict the Sentiments of COVID-19 Vaccination tweets. The data I have used is collecting tweets on the topic \"Covid-19 Vaccination\" (web scraping) and preparing the data. The data was gathered from Twitter and I'm going to use the R environment to implement this project. During the pandemic, lots of studies carried out analyses using Twitter data.\n\n## Loading Libraries :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(twitteR) #R package which provides access to the Twitter API\nlibrary(tm) #Text mining in R\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate) #Lubridate is an R package that makes it easier to work with dates and times.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: timechange\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda) #Makes it easy to manage texts in the form of a corpus.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.4\nUnicode version: 14.0\nICU version: 70.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tm':\n\n    stopwords\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud) #Visualize differences and similarity between documents\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud2)\nlibrary(ggplot2) #For creating Graphics \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:NLP':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(reshape2) # Transform data between wide and long formats.\nlibrary(dplyr) #Provides a grammar of data manipulation\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:twitteR':\n\n    id, location\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse) #Helps to transform and tidy data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ tibble  3.1.8     ✔ purrr   0.3.5\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::annotate()      masks NLP::annotate()\n✖ lubridate::as.difftime() masks base::as.difftime()\n✖ lubridate::date()        masks base::date()\n✖ dplyr::filter()          masks stats::filter()\n✖ dplyr::id()              masks twitteR::id()\n✖ lubridate::intersect()   masks base::intersect()\n✖ dplyr::lag()             masks stats::lag()\n✖ dplyr::location()        masks twitteR::location()\n✖ lubridate::setdiff()     masks base::setdiff()\n✖ lubridate::union()       masks base::union()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext) #Applies the principles of the tidyverse to analyzing text.\nlibrary(tidyr) #Helps to get tidy data\nlibrary(gridExtra) #Arrange multiple grid-based plots on a page, and draw tables\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(grid) #Produce graphical output\nlibrary(rtweet) #Collecting Twitter Data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nThe following object is masked from 'package:twitteR':\n\n    lookup_statuses\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet) #For sentiment scores and emotion classification\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'syuzhet'\n\nThe following object is masked from 'package:rtweet':\n\n    get_tokens\n```\n:::\n\n```{.r .cell-code}\nlibrary(corpus)\nlibrary(\"igraph\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'igraph'\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\nThe following object is masked from 'package:base':\n\n    union\n```\n:::\n\n```{.r .cell-code}\nlibrary(\"knitr\")\nlibrary(\"slam\")\nlibrary(NLP)\nlibrary(cleanNLP)\nlibrary(corpus)\nlibrary(SnowballC)\nlibrary(topicmodels)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(sentimentr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'sentimentr'\n\nThe following object is masked from 'package:syuzhet':\n\n    get_sentences\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(plotrix)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plotrix'\n\nThe following object is masked from 'package:syuzhet':\n\n    rescale\n```\n:::\n\n```{.r .cell-code}\nlibrary(radarchart)\nlibrary(textdata)\nlibrary(ggeasy)\nlibrary(glue)\nlibrary(networkD3)\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n```\n:::\n:::\n\n\n## Scraping Data from Twitter\nAfter getting access to the Twitter API I can run the following (replacing \\###### by my specific credentials) and search for tweets. (\"\\######\" used for protection)\n\n::: {.cell}\n\n```{.r .cell-code}\n# twitter keys and tokens\napi_key <- \"######\"\napi_secret <- \"######\"\naccess_token <- \"######\"\naccess_token_secret <- \"######\"\n\n# create token for rtweet\ntoken <- create_token(\n  app = \"######\",\n  api_key,\n  api_secret,\n  access_token,\n  access_token_secret,\n  set_renv = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `create_token()` was deprecated in rtweet 1.0.0.\nℹ See vignette('auth') for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSaving auth to '/Users/kaushika/Library/Preferences/org.R-project.R/R/rtweet/\ncreate_token.rds'\n```\n:::\n\n```{.r .cell-code}\nsetup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Using direct authentication\"\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in check_twitter_oauth(): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\n#what to search\n\n#Searching for tweets using terms covid + 19 + vaccine and filtering out the retweets to avoid repetitions. After that I converted the list of tweets into a data frame.\n\ntweets_covid = searchTwitter(\"covid+19+vaccine -filter:retweets\", n = 20000, lang = \"en\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twInterfaceObj$doAPICall(cmd, params, \"GET\", ...): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\ntweets.df = twListToDF(tweets_covid)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twListToDF(tweets_covid): object 'tweets_covid' not found\n```\n:::\n\n```{.r .cell-code}\nfor (i in 1:nrow(tweets.df)) {\n    if (tweets.df$truncated[i] == TRUE) {\n        tweets.df$text[i] <- gsub(\"[[:space:]]*$\",\"...\",tweets.df$text[i])\n    }\n}\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(tweets.df): object 'tweets.df' not found\n```\n:::\n\n```{.r .cell-code}\n#Saving the collected tweets into a csv file.\nwrite.csv(tweets.df, file = \"covidtweets.csv\", row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'tweets.df' not found\n```\n:::\n:::\n\n\nReading in the data we scraped :\nThe csv file has approximately 15,000 tweets on the topic \"Covid 19 Vaccination\".\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_19_vaccination <- read.csv(\"covidtweets.csv\", header = T)\nstr(covid_19_vaccination)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t15040 obs. of  16 variables:\n $ text         : chr  \"@1goodtern Who suffer the most, vaccine and mask 😷 off, not thinking long term effects with COVID-19 being a m\"| __truncated__ \"@palminder1990 Google much?\\nhttps://t.co/SXOBS5INdJ\" \"Arrest #JoeBiden for the assault on the #american people forcing and conning them to take the #vaccine for… htt\"| __truncated__ \"@9NewsSyd Remember that time \\\"conspiracy theorists\\\" said that the Covid-19 Vaccine was undertested, wouldn't \"| __truncated__ ...\n $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ favoriteCount: int  0 0 0 0 0 0 0 2 0 0 ...\n $ replyToSN    : chr  \"1goodtern\" \"palminder1990\" NA \"9NewsSyd\" ...\n $ created      : chr  \"2022-10-31 01:35:17\" \"2022-10-31 01:33:07\" \"2022-10-31 01:27:07\" \"2022-10-31 01:24:45\" ...\n $ truncated    : logi  TRUE FALSE TRUE TRUE TRUE TRUE ...\n $ replyToSID   : num  1.59e+18 1.59e+18 NA 1.59e+18 NA ...\n $ id           : num  1.59e+18 1.59e+18 1.59e+18 1.59e+18 1.59e+18 ...\n $ replyToUID   : num  9.61e+17 1.49e+18 NA 1.72e+08 NA ...\n $ statusSource : chr  \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" ...\n $ screenName   : chr  \"ecmoyer\" \"henri_gg\" \"Twitgovbot\" \"DjrellAZDelta\" ...\n $ retweetCount : int  0 0 0 0 0 0 0 0 0 0 ...\n $ isRetweet    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ retweeted    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ longitude    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ latitude     : num  NA NA NA NA NA NA NA NA NA NA ...\n```\n:::\n:::\n\n#Tidying Data :\n\n::: {.cell}\n\n```{.r .cell-code}\n#Suppress warnings in the global setting.\noptions(warn=-1)\n```\n:::\n\n# Pre-Processing\n\n## Text Mining Functions\n\n::: {.cell}\n\n```{.r .cell-code}\n# clean text\nremoveUsername <- function(x) gsub('@[^[:space:]]*', '', x) #Removes usernames\nremoveURL <- function(x) gsub('http[[:alnum:]]*', '', x) #Removes URLs attached to tweets\nremoveNumPunct<- function(x) gsub(\"[^[:alpha:][:space:]]*\",\"\",x) #Remove Punctuations\n\n#Text Mining Functions\ncleandata <- tm_map(corpus, PlainTextDocument) #Function to create plain text documents.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeUsername)) #Function to remove Usernames attached to the text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeUsername)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeURL)) #Function to remove URLs attached to the text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeURL)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(tolower)) #Function to convert text into lowercase.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(tolower)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeNumPunct)) #Function to remove Punctuations attached to text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeNumPunct)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeNumbers)) # #Function to remove Numbers attached to texts.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeNumbers)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, removeWords, stopwords(\"english\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, removeWords, stopwords(\"english\")): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\n#Removing meaningless words like \"covid,\" \"vaccination,\" \"corona,\" etc\ncleandata <- tm_map(cleandata, removeWords, c('covid','vaccination', \n                                            'vaccinations','vaccine','vaccines',\n                                            'vaccinated', \"corona\", \n                                            \"coronavirus\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, removeWords, c(\"covid\", \"vaccination\", \"vaccinations\", : object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, stripWhitespace) #Function to strip extra whitespace from a text document.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, stripWhitespace): object 'cleandata' not found\n```\n:::\n:::\n\n## Corpus\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- Corpus(VectorSource(cleandata))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in SimpleSource(length = length(x), content = x, class = \"VectorSource\"): object 'cleandata' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- tm_map(corpus, removeWords, stopwords(\"en\"))  \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Remove numbers. This could have been done earlier, of course.\ncorpus <- tm_map(corpus, removeNumbers)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Stem the words. Google if you don't understand\ncorpus <- tm_map(corpus, stemDocument)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Remove the stems associated with our search terms!\ncorpus <- tm_map(corpus, removeWords, c(\"covid\", \"vaccine\", \"get\", \"can\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_tweetsdf <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = FALSE)\n#unlist list column \nnew_tweetsdf <- unlist(new_tweetsdf)\n```\n:::\n\n\n\n## Document Term Matrix\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now for Topic Modeling\n\n# Get the lengths and make sure we only create a DTM for tweets with\n# some actual content\ndoc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ndtm <- DocumentTermMatrix(corpus[doc.lengths > 0])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in TermDocumentMatrix(x, control): object 'doc.lengths' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ndtm <- as.matrix(dtm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(dtm): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n## Wordlcoud\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(repr.plot.width=15, repr.plot.height=15)\npal <- brewer.pal(8, \"Dark2\")\nwordcloud(corpus, min.freq=50, max.words = 150, random.order = TRUE, col = pal)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# row sums\nw <- rowSums(dtm) # how often appears each word?\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nw <- subset(w, w>=3000)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in subset(w, w >= 3000): object 'w' not found\n```\n:::\n\n```{.r .cell-code}\nw <- sort(rowSums(dtm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n# wordcloud\noptions(repr.plot.width=14, repr.plot.height=15)\nwordcloud(words = names(w),\n          freq = w,\n          colors=brewer.pal(8, \"Dark2\"),\n          random.color = TRUE,\n          max.words = 100,\n          scale = c(4, 0.04))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in wordcloud(words = names(w), freq = w, colors = brewer.pal(8, : object 'w' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- sort(rowSums(dtm), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nw <- data.frame(names(w), w)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(names(w), w): object 'w' not found\n```\n:::\n\n```{.r .cell-code}\ncolnames(w) <- c('word', 'freq')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in colnames(w) <- c(\"word\", \"freq\"): object 'w' not found\n```\n:::\n\n```{.r .cell-code}\nwordcloud2(w,\n           size = 0.7,\n           shape = 'triangle',\n           rotateRatio = 0.5,\n           minSize = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"table\" %in% class(data): object 'w' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsents = levels(factor(covid_19_vaccination$sentiment))\n```\n:::\n\n\n## Word Frequency \n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- sort(rowSums(dtm), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\ndtm <- data.frame(word = names(dtm), freq = dtm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(word = names(dtm), freq = dtm): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(dtm[1:20,], aes(x=reorder(word, freq), y=freq)) + \n  geom_bar(stat=\"identity\") +\n  xlab(\"Terms\") + \n  ylab(\"Count\") + \n  coord_flip() +\n  theme(axis.text=element_text(size=7)) +\n  ggtitle('Most common word frequency plot') +\n  ggeasy::easy_center_title()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(dtm[1:20, ], aes(x = reorder(word, freq), y = freq)): object 'dtm' not found\n```\n:::\n:::\n\nBigram analysis and Network definition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.words <- covid_19_vaccination %>% \n  unnest_tokens(\n    input = text, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbi.gram.words %>% \n  select(bigram) %>% \n  head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          bigram\n1  1goodtern who\n2     who suffer\n3     suffer the\n4       the most\n5   most vaccine\n6    vaccine and\n7       and mask\n8       mask off\n9        off not\n10  not thinking\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nextra.stop.words <- c('https', 'covid', '19', 'vaccine')\nstopwords.df <- tibble(\n  word = c(stopwords(kind = 'es'),\n           stopwords(kind = 'en'),\n           extra.stop.words)\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopwords(kind = \"es\"): unused argument (kind = \"es\")\n```\n:::\n:::\n\n\nNext, we filter for stop words and remove white spaces.\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.words %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stopwords.df$word) %>% \n  filter(! word2 %in% stopwords.df$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `filter()`:\n! Problem while computing `..1 = !word1 %in% stopwords.df$word`.\nCaused by error in `word1 %in% stopwords.df$word`:\n! object 'stopwords.df' not found\n```\n:::\n:::\n\n\nFinally, we group and count by bigram.\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.count <- bi.gram.words %>% \n  dplyr::count(word1, word2, sort = TRUE) %>% \n  dplyr::rename(weight = n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `group_by()`:\n! Must group by variables found in `.data`.\nColumn `word1` is not found.\nColumn `word2` is not found.\n```\n:::\n\n```{.r .cell-code}\nbi.gram.count %>% head()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'bi.gram.count' not found\n```\n:::\n:::\n\nLet us plot the distribution of the weightvalues:\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.count %>% \n  ggplot(mapping = aes(x = weight)) +\n  theme_light() +\n  geom_histogram() +\n  labs(title = \"Bigram Weight Distribution\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(., mapping = aes(x = weight)): object 'bi.gram.count' not found\n```\n:::\n:::\n\nNote that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.count %>% \n  mutate(weight = log(weight + 1)) %>% \n  ggplot(mapping = aes(x = weight)) +\n  theme_light() +\n  geom_histogram() +\n  labs(title = \"Bigram log-Weight Distribution\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., weight = log(weight + 1)): object 'bi.gram.count' not found\n```\n:::\n:::\n\n## Network Analysis\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 50\n\n# For visualization purposes we scale by a global factor. \nScaleWeight <- function(x, lambda) {\n  x / lambda\n}\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% \n  graph_from_data_frame(directed = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., weight > threshold): object 'bi.gram.count' not found\n```\n:::\n\n```{.r .cell-code}\nplot(\n  network, \n  vertex.size = 1,\n  vertex.label.color = 'black', \n  vertex.label.cex = 0.7, \n  vertex.label.dist = 1,\n  edge.color = 'gray', \n  main = 'Bigram Count Network', \n  sub = glue('Weight Threshold: {threshold}'), \n  alpha = 50\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(network, vertex.size = 1, vertex.label.color = \"black\", : object 'network' not found\n```\n:::\n:::\n\nWe can even improvise the representation by setting the sizes of the nodes and the edges by the degree and weight respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nV(network)$degree <- strength(graph = network)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Compute the weight shares.\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\nplot(\n  network, \n  vertex.color = 'lightblue',\n  # Scale node size by degree.\n  vertex.size = 2*V(network)$degree,\n  vertex.label.color = 'black', \n  vertex.label.cex = 0.6, \n  vertex.label.dist = 1.6,\n  edge.color = 'gray', \n  # Set edge width proportional to the weight relative value.\n  edge.width = 3*E(network)$width ,\n  main = 'Bigram Count Network', \n  sub = glue('Weight Threshold: {threshold}'), \n  alpha = 50\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(network, vertex.color = \"lightblue\", vertex.size = 2 * V(network)$degree, : object 'network' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 50\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., weight > threshold): object 'bi.gram.count' not found\n```\n:::\n\n```{.r .cell-code}\n# Store the degree.\nV(network)$degree <- strength(graph = network)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Compute the weight shares.\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(g): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., Degree = (0.01) * V(network)$degree): object 'network.D3' not found\n```\n:::\n\n```{.r .cell-code}\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., Group = 1): object 'network.D3' not found\n```\n:::\n\n```{.r .cell-code}\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(Source): object 'network.D3' not found\n```\n:::\n:::\n\n\n#Sentimental Analysis :\nSentiment analysis, also known as opinion mining or emotion AI, is the process of analyzing pieces of writing to determine the emotional tone they carry, whether their sentiment is positive or negative or even if their primary emotion is angry, sad, surprised etc. Sentiment analysis helps to find the author’s attitude towards a topic.\n\nImport Sentiment Lexicons\nTo be able to categorize the words in our data (wether they are positive, negative, etc.), we need a dictionary resp. a sentiment lexicon that computes the sentiment of a word by analyzing the \"semantic orientation\" of that word in a text. These codings are made by people, through crowdsorcing, etc. For English pieces of writing we can use the following dictionaries:\n\nAfinn: Gives each word a number between [-5, 5], where -5 means that the words is very negative and 5 means that the words is very positive\nBing: Gives each word an assignment of positive/negative sentiment\nNRC: Assigns the words one of the eight primary emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (positive and negative)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafinn <- read_csv(\"Afinn.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 2477 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): word\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nbing <- read_csv(\"Bing.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 6786 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): word, sentiment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nnrc <- read_csv(\"NRC.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 13901 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): word, sentiment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# positive-negative-word cloud\nunnest_tweets <- covid_19_vaccination %>% \n  mutate(text = as.character(covid_19_vaccination$text)) %>% \n  unnest_tokens(word, text)\n\noptions(repr.plot.width=4, repr.plot.height=2)\nunnest_tweets %>% \n  inner_join(bing, by=\"word\") %>%\n  count(word, sentiment, sort=T) %>% \n  acast(word ~ sentiment, value.var = \"n\", fill=0) %>% \n  \n  # wordcloud\n  comparison.cloud(colors=c(\"#DB5656\",\"#DBA656\"), \n                   max.words = 100, \n                   title.size = 2.5,\n                   scale = c(2,0.9))\n```\n\n::: {.cell-output-display}\n![](BlogPost5_Kaushika-Potluri_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "BlogPost5_Kaushika-Potluri_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}