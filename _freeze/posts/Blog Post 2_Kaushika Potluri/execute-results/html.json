{
  "hash": "eb076ed76507fddb5f2c28ba29644149",
  "result": {
    "markdown": "---\ntitle: \"Blog Post 2\"\nauthor: \"Kaushika\"\ndesription: \"Sentimental Analysis on Covid 19 Vaccine\"\ndate: \"10/02/2022\"\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-copy: true\n    code-tools: true\ncategories:\n  - blog Post 2\n  - Kaushika \n---\n\n\n### Data sources used:\n\nIn this project, I am going to predict the Sentiments of COVID-19 Vaccination tweets. The data I have used is collecting tweets on the topic \"Covid-19 Vaccination\" (web scraping) and preparing the data. The data was gathered from Twitter and I'm going to use the R environment to implement this project. During the pandemic, lots of studies carried out analyses using Twitter data.\n\nI have currently scraped the data from Twitter however I have only got tweets from the last 7 days since Twitter only allows me to do so. However, I will keep collecting data or try getting access to Twitter API for Academic Research which will allow me to get tweets from any timeline. So that would better help me visualize my data without any bias.\n\nTo connect to the Twitter API I have used two libraries twitteR and rtweet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(twitteR) #R package which provides access to the Twitter API\nlibrary(tm) #Text mining in R\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate) #Lubridate is an R package that makes it easier to work with dates and times.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: timechange\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud) #Visualize differences and similarity between documents\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud2)\nlibrary(ggplot2) #For creating Graphics \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:NLP':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(reshape2) # Transform data between wide and long formats.\nlibrary(dplyr) #Provides a grammar of data manipulation\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:twitteR':\n\n    id, location\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse) #Helps to transform and tidy data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ tibble  3.1.8     ✔ purrr   0.3.5\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::annotate()      masks NLP::annotate()\n✖ lubridate::as.difftime() masks base::as.difftime()\n✖ lubridate::date()        masks base::date()\n✖ dplyr::filter()          masks stats::filter()\n✖ dplyr::id()              masks twitteR::id()\n✖ lubridate::intersect()   masks base::intersect()\n✖ dplyr::lag()             masks stats::lag()\n✖ dplyr::location()        masks twitteR::location()\n✖ lubridate::setdiff()     masks base::setdiff()\n✖ lubridate::union()       masks base::union()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext) #Applies the principles of the tidyverse to analyzing text.\nlibrary(tidyr) #Helps to get tidy data\nlibrary(gridExtra) #Arrange multiple grid-based plots on a page, and draw tables\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(grid) #Produce graphical output\nlibrary(rtweet) #Collecting Twitter Data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nThe following object is masked from 'package:twitteR':\n\n    lookup_statuses\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'syuzhet'\n\nThe following object is masked from 'package:rtweet':\n\n    get_tokens\n```\n:::\n:::\n\n\nIn order to gain access to Twitter data, I will have to apply for a developer account. I will need first to establish a secure connection to the Twitter API; for the connection, I need to provide a consumer API key and a consumer API secret. I can obtain these two by creating a developer profile with Twitter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# twitter keys and tokens\napi_key <- \"######\"\napi_secret <- \"######\"\naccess_token <- \"######\"\naccess_token_secret <- \"######\"\n\n# create token for rtweet\ntoken <- create_token(\n  app = \"######\",\n  api_key,\n  api_secret,\n  access_token,\n  access_token_secret,\n  set_renv = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `create_token()` was deprecated in rtweet 1.0.0.\nℹ See vignette('auth') for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSaving auth to '/Users/kaushika/Library/Preferences/org.R-project.R/R/rtweet/\ncreate_token.rds'\n```\n:::\n:::\n\n\nTo start, we need to establish the connection.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetup_twitter_oauth(api_key, api_secret, access_token, access_token_secret) #Authorising the connection\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Using direct authentication\"\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in check_twitter_oauth(): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n:::\n\n\nI will then use a basic function called searchTwitter to find tweets using multiple criteria.\n\nFor example, I have used the searchTwitter criteria to find 10000 tweets on the topic Covid-19 vaccine to best evaluate tweets related to that. I have also limited our scope in English and can give a time restriction. I converted the returned tweets into a data frame using the function twListToDF. Moreover, I noticed that all of the example tweets have RT at the beginning. This implies those results are retweeted and I filtered out those retweets using the function -filter:retweets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets_covid = searchTwitter(\"covid+19+vaccine -filter:retweets\", n = 10000, lang = \"en\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twInterfaceObj$doAPICall(cmd, params, \"GET\", ...): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\ntweets.df = twListToDF(tweets_covid)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twListToDF(tweets_covid): object 'tweets_covid' not found\n```\n:::\n\n```{.r .cell-code}\nwrite.csv(tweets.df, file = \"covid197tweets.csv\", row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'tweets.df' not found\n```\n:::\n:::\n\n\nWe can write our data frame into a CSV file and observe that the text feature is complete.\n\n### Build Corpus\n\nA corpus, or an aggregate of text documents or tweets, is the primary document management structure in the R package \"tm\" (text mining).\n\n## Data Pre-Processing\n\n### Cleaning the Data\n\nCleaning the data include removing stopwords, numbers, punctuation, and other elements. Stopwords are words that have no sentimental meaning, such as conjunctions, pronouns, negations, etc. Common yet meaningless words like \"covid,\" \"vaccination,\" \"corona,\" etc. are also omitted in this context. The pre-processing of the text data is an essential step as it makes the raw text ready for mining.\n\n## Social Network Analysis\n\n### Analysis of the Most Frequent Words - Word Cloud\n\nA collection of words presented in various sizes is called a wordcloud. The bigger and bolder the word appears, the more frequently a term is used in tweets.\n\n### Research Question\n\nI specifically focused on tweets about COVID-19 vaccines. I wish to perform a Sentiment Analysis on tweets related to the Covid-19 Vaccine. In the first part, I wish to collect tweets related to the Covid-19 vaccine (Web scraping) and prepare the data.\n\nIn the next part, I wish to conduct a social network analysis and visualize the underlying emotions (sentiments) of the tweets.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}