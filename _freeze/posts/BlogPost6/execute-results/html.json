{
  "hash": "b5685a87290890f4a2acab848ddf59f2",
  "result": {
    "markdown": "##Loading important libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(twitteR) #R package which provides access to the Twitter API\nlibrary(tm) #Text mining in R\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate) #Lubridate is an R package that makes it easier to work with dates and times.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: timechange\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda) #Makes it easy to manage texts in the form of a corpus.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.4\nUnicode version: 14.0\nICU version: 70.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tm':\n\n    stopwords\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud) #Visualize differences and similarity between documents\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud2)\nlibrary(ggplot2) #For creating Graphics \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:NLP':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(reshape2) # Transform data between wide and long formats.\nlibrary(dplyr) #Provides a grammar of data manipulation\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:twitteR':\n\n    id, location\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse) #Helps to transform and tidy data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nâ”€â”€ Attaching packages\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntidyverse 1.3.2 â”€â”€\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nâœ” tibble  3.1.8     âœ” purrr   0.3.5\nâœ” tidyr   1.2.1     âœ” stringr 1.4.1\nâœ” readr   2.1.3     âœ” forcats 0.5.2\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– ggplot2::annotate()      masks NLP::annotate()\nâœ– lubridate::as.difftime() masks base::as.difftime()\nâœ– lubridate::date()        masks base::date()\nâœ– dplyr::filter()          masks stats::filter()\nâœ– dplyr::id()              masks twitteR::id()\nâœ– lubridate::intersect()   masks base::intersect()\nâœ– dplyr::lag()             masks stats::lag()\nâœ– dplyr::location()        masks twitteR::location()\nâœ– lubridate::setdiff()     masks base::setdiff()\nâœ– lubridate::union()       masks base::union()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext) #Applies the principles of the tidyverse to analyzing text.\nlibrary(tidyr) #Helps to get tidy data\nlibrary(gridExtra) #Arrange multiple grid-based plots on a page, and draw tables\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(grid) #Produce graphical output\nlibrary(rtweet) #Collecting Twitter Data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nThe following object is masked from 'package:twitteR':\n\n    lookup_statuses\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet) #Returns a data frame in which each row represents a sentence from the original file\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'syuzhet'\n\nThe following object is masked from 'package:rtweet':\n\n    get_tokens\n```\n:::\n\n```{.r .cell-code}\nlibrary(topicmodels)\n```\n:::\n\n\n## Scraping Data from Twitter\n\nAfter getting access to the Twitter API I can run the following (replacing \\###### by my specific credentials) and search for tweets. (\"\\######\" used for protection)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# twitter keys and tokens\napi_key <- \"cHsX0LBreWzbkShT6BugSaKu4\"\napi_secret <- \"CjzTxvRQkoKasibIZge92oqlwLTjWAWqLSKnH6NTZEg7xuURpQ\"\naccess_token <- \"1574938226623651842-pzGlR6Tr5lH3OkSeFsBERQkqTqxv2E\"\naccess_token_secret <- \"cpVs1xq4NibJ2usnYUCaxcpRI9kfDSSA92tXCCZKd8NJQ\"\n\n# create token for rtweet\ntoken <- create_token(\n  app = \"25576157\",\n  api_key,\n  api_secret,\n  access_token,\n  access_token_secret,\n  set_renv = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `create_token()` was deprecated in rtweet 1.0.0.\nâ„¹ See vignette('auth') for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSaving auth to '/Users/kaushika/Library/Preferences/org.R-project.R/R/rtweet/\ncreate_token.rds'\n```\n:::\n\n```{.r .cell-code}\nsetup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Using direct authentication\"\n```\n:::\n\n```{.r .cell-code}\n#what to search\n\n#Searching for tweets using terms covid + 19 + vaccine and filtering out the retweets to avoid repetitions. After that I converted the list of tweets into a data frame.\n\ntweets_covid = searchTwitter(\"covid+19+vaccine -filter:retweets\", n = 20000, lang = \"en\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Rate limited .... blocking for a minute and retrying up to 119 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 118 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 117 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 116 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 115 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 114 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 113 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 112 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 111 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 110 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 109 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 108 times ...\"\n[1] \"Rate limited .... blocking for a minute and retrying up to 107 times ...\"\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in doRppAPICall(\"search/tweets\", n, params = params, retryOnRateLimit\n= retryOnRateLimit, : 20000 tweets were requested but the API can only return\n18726\n```\n:::\n\n```{.r .cell-code}\ntweets.df = twListToDF(tweets_covid)\n\nfor (i in 1:nrow(tweets.df)) {\n    if (tweets.df$truncated[i] == TRUE) {\n        tweets.df$text[i] <- gsub(\"[[:space:]]*$\",\"...\",tweets.df$text[i])\n    }\n}\n\n#Saving the collected tweets into a csv file.\nwrite.csv(tweets.df, file = \"covidtweetsbro.csv\", row.names = FALSE)\n```\n:::\n\n\n## Reading the csv file\n\nThe csv file has approximately 15,000 tweets on the topic \"Covid 19 Vaccination\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_19_vaccination <- read.csv(\"covidtweets.csv\", header = T)\nstr(covid_19_vaccination)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t15040 obs. of  16 variables:\n $ text         : chr  \"@1goodtern Who suffer the most, vaccine and mask ðŸ˜· off, not thinking long term effects with COVID-19 being a m\"| __truncated__ \"@palminder1990 Google much?\\nhttps://t.co/SXOBS5INdJ\" \"Arrest #JoeBiden for the assault on the #american people forcing and conning them to take the #vaccine forâ€¦ htt\"| __truncated__ \"@9NewsSyd Remember that time \\\"conspiracy theorists\\\" said that the Covid-19 Vaccine was undertested, wouldn't \"| __truncated__ ...\n $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ favoriteCount: int  0 0 0 0 0 0 0 2 0 0 ...\n $ replyToSN    : chr  \"1goodtern\" \"palminder1990\" NA \"9NewsSyd\" ...\n $ created      : chr  \"2022-10-31 01:35:17\" \"2022-10-31 01:33:07\" \"2022-10-31 01:27:07\" \"2022-10-31 01:24:45\" ...\n $ truncated    : logi  TRUE FALSE TRUE TRUE TRUE TRUE ...\n $ replyToSID   : num  1.59e+18 1.59e+18 NA 1.59e+18 NA ...\n $ id           : num  1.59e+18 1.59e+18 1.59e+18 1.59e+18 1.59e+18 ...\n $ replyToUID   : num  9.61e+17 1.49e+18 NA 1.72e+08 NA ...\n $ statusSource : chr  \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" ...\n $ screenName   : chr  \"ecmoyer\" \"henri_gg\" \"Twitgovbot\" \"DjrellAZDelta\" ...\n $ retweetCount : int  0 0 0 0 0 0 0 0 0 0 ...\n $ isRetweet    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ retweeted    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ longitude    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ latitude     : num  NA NA NA NA NA NA NA NA NA NA ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Suppress warnings in the global setting.\noptions(warn=-1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# clean text\nremoveUsername <- function(x) gsub('@[^[:space:]]*', '', x) #Removes usernames\nremoveURL <- function(x) gsub('http[[:alnum:]]*', '', x) #Removes URLs attached to tweets\nremoveNumPunct<- function(x) gsub(\"[^[:alpha:][:space:]]*\",\"\",x) #Remove Punctuations\n\n#Text Mining Functions\ncleandata <- tm_map(corpus, PlainTextDocument) #Function to create plain text documents.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeUsername)) #Function to remove Usernames attached to the text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeUsername)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeURL)) #Function to remove URLs attached to the text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeURL)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(tolower)) #Function to convert text into lowercase.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(tolower)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeNumPunct)) #Function to remove Punctuations attached to text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeNumPunct)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeNumbers)) # #Function to remove Numbers attached to texts.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeNumbers)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, removeWords, stopwords(\"english\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, removeWords, stopwords(\"english\")): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\n#Removing meaningless words like \"covid,\" \"vaccination,\" \"corona,\" etc\ncleandata <- tm_map(cleandata, removeWords, c('covid','vaccination', \n                                            'vaccinations','vaccine','vaccines',\n                                            'vaccinated', \"corona\", \n                                            \"coronavirus\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, removeWords, c(\"covid\", \"vaccination\", \"vaccinations\", : object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, stripWhitespace) #Function to strip extra whitespace from a text document.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, stripWhitespace): object 'cleandata' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- Corpus(VectorSource(cleandata))  # Create corpus object\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in SimpleSource(length = length(x), content = x, class = \"VectorSource\"): object 'cleandata' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- tm_map(corpus, removeWords, stopwords(\"en\"))  \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Remove numbers. This could have been done earlier, of course.\ncorpus <- tm_map(corpus, removeNumbers)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Stem the words. Google if you don't understand\ncorpus <- tm_map(corpus, stemDocument)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Remove the stems associated with our search terms!\ncorpus <- tm_map(corpus, removeWords, c(\"covid\", \"vaccine\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mar = c(11,11,11,11))\n\n\npal <- brewer.pal(8, \"Dark2\")\nwordcloud(corpus, min.freq=50, max.words = 100, random.order = TRUE, col = pal)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now for Topic Modeling\n\n# Get the lengths and make sure we only create a DTM for tweets with\n# some actual content\ndoc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ndtm <- DocumentTermMatrix(corpus[doc.lengths > 0])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in TermDocumentMatrix(x, control): object 'doc.lengths' not found\n```\n:::\n\n```{.r .cell-code}\n# model <- LDA(dtm, 10)  # Go ahead and test a simple model if you want\ndtm\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninspect(dtm[1:2,10:15])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in inspect(dtm[1:2, 10:15]): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq = colSums(as.matrix(dtm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'colSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nlength(freq)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'freq' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nord = order(freq, decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in order(freq, decreasing = TRUE): object 'freq' not found\n```\n:::\n\n```{.r .cell-code}\nfreq[head(ord, n = 20)]\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'freq' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(dtm, \"health\",0.2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in findAssocs(dtm, \"health\", 0.2): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(dtm, \"pfizer\",0.2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in findAssocs(dtm, \"pfizer\", 0.2): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot = data.frame(words = names(freq), count = freq)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(words = names(freq), count = freq): object 'freq' not found\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nplot = subset(plot, plot$count > 150) #creating a subset of words having more than 100 frequency\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot$count: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\nstr(plot)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (x, y, ...)  \n```\n:::\n\n```{.r .cell-code}\nggplot(data = plot, aes(words, count)) + geom_bar(stat = 'identity') + ggtitle('Words used more than 150 times')+coord_flip()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `ggplot()`:\n! `data` cannot be a function.\nâ„¹ Have you misspelled the `data` argument in `ggplot()`\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(topicmodels)\n#LDA model with 5 topics selected\nlda_5 = LDA(dtm, k = 5, method = 'Gibbs', \n          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, \n                         thin = 500, burnin = 4000, iter = 2000))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n#LDA model with 2 topics selected\nlda_2 = LDA(dtm, k = 2, method = 'Gibbs', \n          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, \n                         thin = 500, burnin = 4000, iter = 2000))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n#LDA model with 10 topics selected\nlda_10 = LDA(dtm, k = 10, method = 'Gibbs', \n          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, \n                         thin = 500, burnin = 4000, iter = 2000))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Top 10 terms or words under each topic\ntop10terms_5 = as.matrix(terms(lda_5,10))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'terms': object 'lda_5' not found\n```\n:::\n\n```{.r .cell-code}\ntop10terms_2 = as.matrix(terms(lda_2,10))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'terms': object 'lda_2' not found\n```\n:::\n\n```{.r .cell-code}\ntop10terms_10 = as.matrix(terms(lda_10,10))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'terms': object 'lda_10' not found\n```\n:::\n\n```{.r .cell-code}\ntop10terms_5\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top10terms_5' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntop10terms_2\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top10terms_2' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntop10terms_10\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top10terms_10' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlda.topics_5 = as.matrix(topics(lda_5))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'topics': object 'lda_5' not found\n```\n:::\n\n```{.r .cell-code}\nlda.topics_2 = as.matrix(topics(lda_2))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'topics': object 'lda_2' not found\n```\n:::\n\n```{.r .cell-code}\nlda.topics_10 = as.matrix(topics(lda_10))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'topics': object 'lda_10' not found\n```\n:::\n\n```{.r .cell-code}\n#write.csv(lda.topics_5,file = paste('LDAGibbs',5,'DocsToTopics.csv'))\n#write.csv(lda.topics_2,file = paste('LDAGibbs',2,'DocsToTopics.csv'))\n#write.csv(lda.topics_10,file = paste('LDAGibbs',10,'DocsToTopics.csv'))\n\nsummary(as.factor(lda.topics_5[,1]))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'lda.topics_5' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntopicprob_5 = as.matrix(lda_5@gamma)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(lda_5@gamma): object 'lda_5' not found\n```\n:::\n\n```{.r .cell-code}\ntopicprob_2 = as.matrix(lda_2@gamma)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(lda_2@gamma): object 'lda_2' not found\n```\n:::\n\n```{.r .cell-code}\ntopicprob_10 = as.matrix(lda_10@gamma)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(lda_10@gamma): object 'lda_10' not found\n```\n:::\n\n```{.r .cell-code}\n#write.csv(topicprob_5, file = paste('LDAGibbs', 5, 'DoctToTopicProb.csv'))\n#write.csv(topicprob_2, file = paste('LDAGibbs', 2, 'DoctToTopicProb.csv'))\n#write.csv(topicprob_10, file = paste('LDAGibbs', 10, 'DoctToTopicProb.csv'))\n\nhead(topicprob_2,1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(topicprob_2, 1): object 'topicprob_2' not found\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}