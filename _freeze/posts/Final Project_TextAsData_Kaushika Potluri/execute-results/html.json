{
  "hash": "8d7849d7cbba41f1c66d556917259c98",
  "result": {
    "markdown": "---\ntitle: \"Final Project\"\neditor: visual\ndesription: \"Sentiment Analysis and Topic Modeling on Covid-19 Vaccine\"\ndate: \"12/20/2022\"\nformat:\n  html:\n    code-fold: true\n    code-copy: true\n    code-tools: true \n   \ncategories:\n  - Final Project \n  - Kaushika Potluri\n---\n\n\n# Introduction\nOver just the past two years, the COVID-19 outbreak has quickly become a global emergency. While the coronavirus is severely contagious and deadly, people respond to the Covid-19 vaccine with mixed feelings on social media.\nThis project is interested in whether and how people change their attitudes towards the COVID-19 vaccine. Specifically, the project would like to answer the following research questions:\nIn this project, I am going to predict the Sentiments of COVID-19 Vaccination tweets. The data I have used is collecting tweets on the topic \"Covid-19 Vaccination\" (web scraping) and preparing the data. The data was gathered from Twitter and I'm going to use the R environment to implement this project. During the pandemic, lots of studies carried out analyses using Twitter data.\n\n## Loading Libraries :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(twitteR) #R package which provides access to the Twitter API\nlibrary(tm) #Text mining in R\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate) #Lubridate is an R package that makes it easier to work with dates and times.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: timechange\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(quanteda) #Makes it easy to manage texts in the form of a corpus.\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage version: 3.2.4\nUnicode version: 14.0\nICU version: 70.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nParallel computing: 8 of 8 threads used.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSee https://quanteda.io for tutorials and examples.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'quanteda'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:tm':\n\n    stopwords\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:NLP':\n\n    meta, meta<-\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud) #Visualize differences and similarity between documents\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: RColorBrewer\n```\n:::\n\n```{.r .cell-code}\nlibrary(wordcloud2)\nlibrary(ggplot2) #For creating Graphics \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:NLP':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(reshape2) # Transform data between wide and long formats.\nlibrary(dplyr) #Provides a grammar of data manipulation\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:twitteR':\n\n    id, location\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse) #Helps to transform and tidy data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ tibble  3.1.8     ✔ purrr   0.3.5\n✔ tidyr   1.2.1     ✔ stringr 1.4.1\n✔ readr   2.1.3     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::annotate()      masks NLP::annotate()\n✖ lubridate::as.difftime() masks base::as.difftime()\n✖ lubridate::date()        masks base::date()\n✖ dplyr::filter()          masks stats::filter()\n✖ dplyr::id()              masks twitteR::id()\n✖ lubridate::intersect()   masks base::intersect()\n✖ dplyr::lag()             masks stats::lag()\n✖ dplyr::location()        masks twitteR::location()\n✖ lubridate::setdiff()     masks base::setdiff()\n✖ lubridate::union()       masks base::union()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext) #Applies the principles of the tidyverse to analyzing text.\nlibrary(tidyr) #Helps to get tidy data\nlibrary(gridExtra) #Arrange multiple grid-based plots on a page, and draw tables\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\nlibrary(grid) #Produce graphical output\nlibrary(rtweet) #Collecting Twitter Data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rtweet'\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nThe following object is masked from 'package:twitteR':\n\n    lookup_statuses\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet) #For sentiment scores and emotion classification\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'syuzhet'\n\nThe following object is masked from 'package:rtweet':\n\n    get_tokens\n```\n:::\n\n```{.r .cell-code}\nlibrary(corpus)\nlibrary(\"igraph\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'igraph'\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\nThe following object is masked from 'package:base':\n\n    union\n```\n:::\n\n```{.r .cell-code}\nlibrary(\"knitr\")\nlibrary(\"slam\")\nlibrary(NLP)\nlibrary(cleanNLP)\nlibrary(corpus)\nlibrary(SnowballC)\nlibrary(topicmodels)\nlibrary(stringr)\nlibrary(stringi)\nlibrary(sentimentr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'sentimentr'\n\nThe following object is masked from 'package:syuzhet':\n\n    get_sentences\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(plotrix)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plotrix'\n\nThe following object is masked from 'package:syuzhet':\n\n    rescale\n```\n:::\n\n```{.r .cell-code}\nlibrary(radarchart)\nlibrary(textdata)\nlibrary(ggeasy)\nlibrary(glue)\nlibrary(networkD3)\nlibrary(magrittr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n```\n:::\n:::\n\n\n## Scraping Data from Twitter\nAfter getting access to the Twitter API I can run the following (replacing \\###### by my specific credentials) and search for tweets. (\"\\######\" used for protection)\n\n::: {.cell}\n\n```{.r .cell-code}\n# twitter keys and tokens\napi_key <- \"######\"\napi_secret <- \"######\"\naccess_token <- \"######\"\naccess_token_secret <- \"######\"\n\n# create token for rtweet\ntoken <- create_token(\n  app = \"######\",\n  api_key,\n  api_secret,\n  access_token,\n  access_token_secret,\n  set_renv = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `create_token()` was deprecated in rtweet 1.0.0.\nℹ See vignette('auth') for details\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSaving auth to '/Users/kaushika/Library/Preferences/org.R-project.R/R/rtweet/\ncreate_token.rds'\n```\n:::\n\n```{.r .cell-code}\nsetup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Using direct authentication\"\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in check_twitter_oauth(): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\n#what to search\n\n#Searching for tweets using terms covid + 19 + vaccine and filtering out the retweets to avoid repetitions. After that I converted the list of tweets into a data frame.\n\ntweets_covid = searchTwitter(\"covid+19+vaccine -filter:retweets\", n = 20000, lang = \"en\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twInterfaceObj$doAPICall(cmd, params, \"GET\", ...): OAuth authentication error:\nThis most likely means that you have incorrectly called setup_twitter_oauth()'\n```\n:::\n\n```{.r .cell-code}\ntweets.df = twListToDF(tweets_covid)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in twListToDF(tweets_covid): object 'tweets_covid' not found\n```\n:::\n\n```{.r .cell-code}\nfor (i in 1:nrow(tweets.df)) {\n    if (tweets.df$truncated[i] == TRUE) {\n        tweets.df$text[i] <- gsub(\"[[:space:]]*$\",\"...\",tweets.df$text[i])\n    }\n}\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in nrow(tweets.df): object 'tweets.df' not found\n```\n:::\n\n```{.r .cell-code}\n#Saving the collected tweets into a csv file.\nwrite.csv(tweets.df, file = \"covidtweets.csv\", row.names = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.data.frame(x): object 'tweets.df' not found\n```\n:::\n:::\n\n\nReading in the data we scraped :\nThe csv file has approximately 15,000 tweets on the topic \"Covid 19 Vaccination\".\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_19_vaccination <- read.csv(\"covidtweets.csv\", header = T)\nstr(covid_19_vaccination)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t15040 obs. of  16 variables:\n $ text         : chr  \"@1goodtern Who suffer the most, vaccine and mask 😷 off, not thinking long term effects with COVID-19 being a m\"| __truncated__ \"@palminder1990 Google much?\\nhttps://t.co/SXOBS5INdJ\" \"Arrest #JoeBiden for the assault on the #american people forcing and conning them to take the #vaccine for… htt\"| __truncated__ \"@9NewsSyd Remember that time \\\"conspiracy theorists\\\" said that the Covid-19 Vaccine was undertested, wouldn't \"| __truncated__ ...\n $ favorited    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ favoriteCount: int  0 0 0 0 0 0 0 2 0 0 ...\n $ replyToSN    : chr  \"1goodtern\" \"palminder1990\" NA \"9NewsSyd\" ...\n $ created      : chr  \"2022-10-31 01:35:17\" \"2022-10-31 01:33:07\" \"2022-10-31 01:27:07\" \"2022-10-31 01:24:45\" ...\n $ truncated    : logi  TRUE FALSE TRUE TRUE TRUE TRUE ...\n $ replyToSID   : num  1.59e+18 1.59e+18 NA 1.59e+18 NA ...\n $ id           : num  1.59e+18 1.59e+18 1.59e+18 1.59e+18 1.59e+18 ...\n $ replyToUID   : num  9.61e+17 1.49e+18 NA 1.72e+08 NA ...\n $ statusSource : chr  \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" \"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone</a>\" \"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App</a>\" ...\n $ screenName   : chr  \"ecmoyer\" \"henri_gg\" \"Twitgovbot\" \"DjrellAZDelta\" ...\n $ retweetCount : int  0 0 0 0 0 0 0 0 0 0 ...\n $ isRetweet    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ retweeted    : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ longitude    : num  NA NA NA NA NA NA NA NA NA NA ...\n $ latitude     : num  NA NA NA NA NA NA NA NA NA NA ...\n```\n:::\n:::\n\n#Tidying Data :\n\n::: {.cell}\n\n```{.r .cell-code}\n#Suppress warnings in the global setting.\noptions(warn=-1)\n```\n:::\n\n# Pre-Processing\n\n## Text Mining Functions\n\n::: {.cell}\n\n```{.r .cell-code}\n# clean text\nremoveUsername <- function(x) gsub('@[^[:space:]]*', '', x) #Removes usernames\nremoveURL <- function(x) gsub('http[[:alnum:]]*', '', x) #Removes URLs attached to tweets\nremoveNumPunct<- function(x) gsub(\"[^[:alpha:][:space:]]*\",\"\",x) #Remove Punctuations\n\n#Text Mining Functions\ncleandata <- tm_map(corpus, PlainTextDocument) #Function to create plain text documents.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeUsername)) #Function to remove Usernames attached to the text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeUsername)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeURL)) #Function to remove URLs attached to the text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeURL)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(tolower)) #Function to convert text into lowercase.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(tolower)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeNumPunct)) #Function to remove Punctuations attached to text.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeNumPunct)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, content_transformer(removeNumbers)) # #Function to remove Numbers attached to texts.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, content_transformer(removeNumbers)): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, removeWords, stopwords(\"english\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, removeWords, stopwords(\"english\")): object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\n#Removing meaningless words like \"covid,\" \"vaccination,\" \"corona,\" etc\ncleandata <- tm_map(cleandata, removeWords, c('covid','vaccination', \n                                            'vaccinations','vaccine','vaccines',\n                                            'vaccinated', \"corona\", \n                                            \"coronavirus\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, removeWords, c(\"covid\", \"vaccination\", \"vaccinations\", : object 'cleandata' not found\n```\n:::\n\n```{.r .cell-code}\ncleandata <- tm_map(cleandata, stripWhitespace) #Function to strip extra whitespace from a text document.\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in tm_map(cleandata, stripWhitespace): object 'cleandata' not found\n```\n:::\n:::\n\n## Corpus\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- Corpus(VectorSource(cleandata))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in SimpleSource(length = length(x), content = x, class = \"VectorSource\"): object 'cleandata' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus <- tm_map(corpus, removeWords, stopwords(\"en\"))  \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Remove numbers. This could have been done earlier, of course.\ncorpus <- tm_map(corpus, removeNumbers)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Stem the words. Google if you don't understand\ncorpus <- tm_map(corpus, stemDocument)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n\n```{.r .cell-code}\n# Remove the stems associated with our search terms!\ncorpus <- tm_map(corpus, removeWords, c(\"covid\", \"vaccine\", \"get\", \"can\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in UseMethod(\"tm_map\", x): no applicable method for 'tm_map' applied to an object of class \"function\"\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_tweetsdf <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = FALSE)\n#unlist list column \nnew_tweetsdf <- unlist(new_tweetsdf)\n```\n:::\n\n\n\n## Document Term Matrix\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now for Topic Modeling\n\n# Get the lengths and make sure we only create a DTM for tweets with\n# some actual content\ndoc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ndtm <- DocumentTermMatrix(corpus[doc.lengths > 0])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in TermDocumentMatrix(x, control): object 'doc.lengths' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ndtm <- as.matrix(dtm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.matrix(dtm): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n## Wordlcoud\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(repr.plot.width=15, repr.plot.height=15)\npal <- brewer.pal(8, \"Dark2\")\nwordcloud(corpus, min.freq=50, max.words = 150, random.order = TRUE, col = pal)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# row sums\nw <- rowSums(dtm) # how often appears each word?\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nw <- subset(w, w>=3000)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in subset(w, w >= 3000): object 'w' not found\n```\n:::\n\n```{.r .cell-code}\nw <- sort(rowSums(dtm))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n# wordcloud\noptions(repr.plot.width=14, repr.plot.height=15)\nwordcloud(words = names(w),\n          freq = w,\n          colors=brewer.pal(8, \"Dark2\"),\n          random.color = TRUE,\n          max.words = 100,\n          scale = c(4, 0.04))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in wordcloud(words = names(w), freq = w, colors = brewer.pal(8, : object 'w' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- sort(rowSums(dtm), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nw <- data.frame(names(w), w)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(names(w), w): object 'w' not found\n```\n:::\n\n```{.r .cell-code}\ncolnames(w) <- c('word', 'freq')\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in colnames(w) <- c(\"word\", \"freq\"): object 'w' not found\n```\n:::\n\n```{.r .cell-code}\nwordcloud2(w,\n           size = 0.7,\n           shape = 'triangle',\n           rotateRatio = 0.5,\n           minSize = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"table\" %in% class(data): object 'w' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsents = levels(factor(covid_19_vaccination$sentiment))\n```\n:::\n\n\n## Word Frequency \n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- sort(rowSums(dtm), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\ndtm <- data.frame(word = names(dtm), freq = dtm)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(word = names(dtm), freq = dtm): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\nggplot(dtm[1:20,], aes(x=reorder(word, freq), y=freq)) + \n  geom_bar(stat=\"identity\") +\n  xlab(\"Terms\") + \n  ylab(\"Count\") + \n  coord_flip() +\n  theme(axis.text=element_text(size=7)) +\n  ggtitle('Most common word frequency plot') +\n  ggeasy::easy_center_title()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(dtm[1:20, ], aes(x = reorder(word, freq), y = freq)): object 'dtm' not found\n```\n:::\n:::\n\nBigram analysis and Network definition\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.words <- covid_19_vaccination %>% \n  unnest_tokens(\n    input = text, \n    output = bigram, \n    token = 'ngrams', \n    n = 2\n  ) %>% \n  filter(! is.na(bigram))\n\nbi.gram.words %>% \n  select(bigram) %>% \n  head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          bigram\n1  1goodtern who\n2     who suffer\n3     suffer the\n4       the most\n5   most vaccine\n6    vaccine and\n7       and mask\n8       mask off\n9        off not\n10  not thinking\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nextra.stop.words <- c('https', 'covid', '19', 'vaccine')\nstopwords.df <- tibble(\n  word = c(stopwords(kind = 'es'),\n           stopwords(kind = 'en'),\n           extra.stop.words)\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in stopwords(kind = \"es\"): unused argument (kind = \"es\")\n```\n:::\n:::\n\n\nNext, we filter for stop words and remove white spaces.\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.words %<>% \n  separate(col = bigram, into = c('word1', 'word2'), sep = ' ') %>% \n  filter(! word1 %in% stopwords.df$word) %>% \n  filter(! word2 %in% stopwords.df$word) %>% \n  filter(! is.na(word1)) %>% \n  filter(! is.na(word2))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `filter()`:\n! Problem while computing `..1 = !word1 %in% stopwords.df$word`.\nCaused by error in `word1 %in% stopwords.df$word`:\n! object 'stopwords.df' not found\n```\n:::\n:::\n\n\nFinally, we group and count by bigram.\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.count <- bi.gram.words %>% \n  dplyr::count(word1, word2, sort = TRUE) %>% \n  dplyr::rename(weight = n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `group_by()`:\n! Must group by variables found in `.data`.\nColumn `word1` is not found.\nColumn `word2` is not found.\n```\n:::\n\n```{.r .cell-code}\nbi.gram.count %>% head()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in head(.): object 'bi.gram.count' not found\n```\n:::\n:::\n\nLet us plot the distribution of the weightvalues:\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.count %>% \n  ggplot(mapping = aes(x = weight)) +\n  theme_light() +\n  geom_histogram() +\n  labs(title = \"Bigram Weight Distribution\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(., mapping = aes(x = weight)): object 'bi.gram.count' not found\n```\n:::\n:::\n\nNote that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbi.gram.count %>% \n  mutate(weight = log(weight + 1)) %>% \n  ggplot(mapping = aes(x = weight)) +\n  theme_light() +\n  geom_histogram() +\n  labs(title = \"Bigram log-Weight Distribution\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., weight = log(weight + 1)): object 'bi.gram.count' not found\n```\n:::\n:::\n\n## Network Analysis\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 50\n\n# For visualization purposes we scale by a global factor. \nScaleWeight <- function(x, lambda) {\n  x / lambda\n}\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %>% \n  graph_from_data_frame(directed = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., weight > threshold): object 'bi.gram.count' not found\n```\n:::\n\n```{.r .cell-code}\nplot(\n  network, \n  vertex.size = 1,\n  vertex.label.color = 'black', \n  vertex.label.cex = 0.7, \n  vertex.label.dist = 1,\n  edge.color = 'gray', \n  main = 'Bigram Count Network', \n  sub = glue('Weight Threshold: {threshold}'), \n  alpha = 50\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(network, vertex.size = 1, vertex.label.color = \"black\", : object 'network' not found\n```\n:::\n:::\n\nWe can even improvise the representation by setting the sizes of the nodes and the edges by the degree and weight respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nV(network)$degree <- strength(graph = network)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Compute the weight shares.\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\nplot(\n  network, \n  vertex.color = 'lightblue',\n  # Scale node size by degree.\n  vertex.size = 2*V(network)$degree,\n  vertex.label.color = 'black', \n  vertex.label.cex = 0.6, \n  vertex.label.dist = 1.6,\n  edge.color = 'gray', \n  # Set edge width proportional to the weight relative value.\n  edge.width = 3*E(network)$width ,\n  main = 'Bigram Count Network', \n  sub = glue('Weight Threshold: {threshold}'), \n  alpha = 50\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot(network, vertex.color = \"lightblue\", vertex.size = 2 * V(network)$degree, : object 'network' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 50\n\nnetwork <-  bi.gram.count %>%\n  filter(weight > threshold) %>%\n  graph_from_data_frame(directed = FALSE)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in filter(., weight > threshold): object 'bi.gram.count' not found\n```\n:::\n\n```{.r .cell-code}\n# Store the degree.\nV(network)$degree <- strength(graph = network)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Compute the weight shares.\nE(network)$width <- E(network)$weight/max(E(network)$weight)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Create networkD3 object.\nnetwork.D3 <- igraph_to_networkD3(g = network)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(g): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\n# Define node size.\nnetwork.D3$nodes %<>% mutate(Degree = (1E-2)*V(network)$degree)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., Degree = (0.01) * V(network)$degree): object 'network.D3' not found\n```\n:::\n\n```{.r .cell-code}\n# Define color group\nnetwork.D3$nodes %<>% mutate(Group = 1)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in mutate(., Group = 1): object 'network.D3' not found\n```\n:::\n\n```{.r .cell-code}\n# Define edges width. \nnetwork.D3$links$Width <- 10*E(network)$width\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in \"igraph\" %in% class(graph): object 'network' not found\n```\n:::\n\n```{.r .cell-code}\nforceNetwork(\n  Links = network.D3$links, \n  Nodes = network.D3$nodes, \n  Source = 'source', \n  Target = 'target',\n  NodeID = 'name',\n  Group = 'Group', \n  opacity = 0.9,\n  Value = 'Width',\n  Nodesize = 'Degree', \n  # We input a JavaScript function.\n  linkWidth = JS(\"function(d) { return Math.sqrt(d.value); }\"), \n  fontSize = 12,\n  zoom = TRUE, \n  opacityNoHover = 1\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(Source): object 'network.D3' not found\n```\n:::\n:::\n\n\n#Sentimental Analysis :\nSentiment analysis, also known as opinion mining or emotion AI, is the process of analyzing pieces of writing to determine the emotional tone they carry, whether their sentiment is positive or negative or even if their primary emotion is angry, sad, surprised etc. Sentiment analysis helps to find the author’s attitude towards a topic.\n\nImport Sentiment Lexicons\nTo be able to categorize the words in our data (wether they are positive, negative, etc.), we need a dictionary resp. a sentiment lexicon that computes the sentiment of a word by analyzing the \"semantic orientation\" of that word in a text. These codings are made by people, through crowdsorcing, etc. For English pieces of writing we can use the following dictionaries:\n\nAfinn: Gives each word a number between [-5, 5], where -5 means that the words is very negative and 5 means that the words is very positive\nBing: Gives each word an assignment of positive/negative sentiment\nNRC: Assigns the words one of the eight primary emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (positive and negative)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nafinn <- read_csv(\"Afinn.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 2477 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): word\ndbl (1): value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nbing <- read_csv(\"Bing.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 6786 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): word, sentiment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nnrc <- read_csv(\"NRC.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 13901 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): word, sentiment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# positive-negative-word cloud\nunnest_tweets <- covid_19_vaccination %>% \n  mutate(text = as.character(covid_19_vaccination$text)) %>% \n  unnest_tokens(word, text)\n\noptions(repr.plot.width=4, repr.plot.height=2)\nunnest_tweets %>% \n  inner_join(bing, by=\"word\") %>%\n  count(word, sentiment, sort=T) %>% \n  acast(word ~ sentiment, value.var = \"n\", fill=0) %>% \n  \n  # wordcloud\n  comparison.cloud(colors=c(\"#DB5656\",\"#DBA656\"), \n                   max.words = 100, \n                   title.size = 2.5,\n                   scale = c(2,0.9))\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n##Sentiment Scores: Positive and Negative Sentiments\n\nA sentiment score is a scaling system that reflects the emotional depth of emotions in a text. The score makes it simpler to understand how customers feel.\n\nIn the following graph positive and negative sentiments are differentiated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#convert file to utf-8\ntweets <- iconv(covid_19_vaccination$text, to = 'utf-8')\n\n#only positive and negative score \ns <-get_nrc_sentiment(tweets)\ns_only_pos_neg <- select(s,positive,negative)\n\n#calculationg total score for each sentiment\ns_only_pos_neg <- data.frame(colSums(s_only_pos_neg[,]))\n\nnames(s_only_pos_neg) <- \"Score\"\ns_only_pos_neg <- cbind(\"sentiment\"=rownames(s_only_pos_neg), s_only_pos_neg)\nrownames(s_only_pos_neg) <- NULL\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plotting the sentiments with scores\noptions(repr.plot.width=14, repr.plot.height=6)\nggplot(data=s_only_pos_neg,\n       aes(x=sentiment,y=Score))+\n  geom_bar(width = 0.5, aes(fill=sentiment),stat = \"identity\")+\n  theme(legend.position=\"none\")+\n  xlab(\"\")+\n  ylab(\"Score\")+\n  labs(caption = \"Status: 31/10/2022\")+\n  theme(axis.text.x = element_text(size = 20),\n        axis.text.y = element_text(size = 14),\n        axis.title.x = element_text(size = 20),\n        axis.title.y = element_text(size = 20),\n        plot.title = element_text(size = 20)) +\n  scale_fill_manual(values = c(\"red\", \"green\"))+\n  ggtitle(\"\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nPeople are showing more positive than negative emotions for the covid vaccine on Twitter.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\ntweets_bing<-covid_19_vaccination%>% inner_join(get_sentiments(\"bing\")) \n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `inner_join()`:\n! `by` must be supplied when `x` and `y` have no common variables.\nℹ use by = character()` to perform a cross-join.\n```\n:::\n\n```{.r .cell-code}\nperc<-tweets_bing %>% \n  count(sentiment)%>% #count sentiment\n  mutate(total=sum(n)) %>% #get sum\n  group_by(sentiment) %>% #group by sentiment\n  mutate(percent=round(n/total,2)*100) %>% #get the proportion\n  ungroup()\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., sentiment): object 'tweets_bing' not found\n```\n:::\n\n```{.r .cell-code}\nlabel <-c( paste(perc$percent[1],'%',' - ',perc$sentiment[1],sep=''),#create label\n     paste(perc$percent[2],'%',' - ',perc$sentiment[2],sep=''))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in paste(perc$percent[1], \"%\", \" - \", perc$sentiment[1], sep = \"\"): object 'perc' not found\n```\n:::\n\n```{.r .cell-code}\npie3D(perc$percent,labels=label,labelcex=1.1,explode= 0.1, \n      main=\"Worldwide Sentiment\") #create a pie chart\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in pie3D(perc$percent, labels = label, labelcex = 1.1, explode = 0.1, : object 'perc' not found\n```\n:::\n:::\n\nGlobally, people present a relative negative attitude on Twitter during the pandemic.\n\nPlease note: The word \"positive\" can have a different meaning in this context. A \"positive test\" probably has a negative connotation. However, the word \"positive\" is no component of the \"nrc\" sentiment lexicon which is used for the sentiment analysis below. Therefore it has not to be excluded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#remove empty rows leaving aside rows that rows that may start with a space\npattern = \"^[[:space:]]*$\"\nnew_df <- new_tweetsdf[grep(pattern, new_tweetsdf, invert = TRUE)]\n#sentiment analysis\n1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nlibrary(exploratory)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nPackage attached: exploratory v0.3.13. Most recent version available on GitHub: v0.3.16\nYou have an OPTION to update the package by typing 'update_exploratory()'. If you do so, make sure to restart R.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'exploratory'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:readr':\n\n    read_csv\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(devtools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: usethis\n```\n:::\n\n```{.r .cell-code}\nlibrary(sentimentr)\nsentiments_df <- sentiment_attributes(new_df)\nnew_2 <- get_sentences(new_df)\ntweet_sentiment<-sentiment_by(new_2, averaging.function = average_weighted_mixed_sentiment)\n#visualization of sentiments\nlibrary(plotly)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'plotly'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:sentimentr':\n\n    highlight\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:igraph':\n\n    groups\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:graphics':\n\n    layout\n```\n:::\n\n```{.r .cell-code}\n# Make the graph\nsentiment_graph = plot_ly(x=tweet_sentiment$word_count,y=tweet_sentiment$ave_sentiment,mode=\"markers\",colors =c(\"red\",\"yellow\"),size=abs(tweet_sentiment$ave_sentiment)/3 , color=ifelse(tweet_sentiment$ave_sentiment>0,\"Positive\",\"Negative\") ) %>% \n#Change hover mode in the layout argument \nlayout( hovermode=\"closest\",title=\"Sentiment analysis by Tweet\",xaxis= list(title = \"Number of words per Tweet\",size=18),yaxis = list(title = \"Sentiments by Tweet\",size=18))\n# show the graph\nsentiment_graph\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n```\n:::\n\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-1dd2445cdc2d0b3700af\" style=\"width:100%;height:464px;\" class=\"plotly html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-1dd2445cdc2d0b3700af\">{\"x\":{\"visdat\":{\"4c165d9eb2fa\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"4c165d9eb2fa\",\"attrs\":{\"4c165d9eb2fa\":{\"x\":[0,2],\"y\":[0,0],\"mode\":\"markers\",\"color\":[\"Negative\",\"Negative\"],\"size\":[0,0],\"colors\":[\"red\",\"yellow\"],\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20]}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"hovermode\":\"closest\",\"title\":\"Sentiment analysis by Tweet\",\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Number of words per Tweet\",\"size\":18},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"Sentiments by Tweet\",\"size\":18},\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[0,2],\"y\":[0,0],\"mode\":\"markers\",\"type\":\"scatter\",\"name\":\"Negative\",\"marker\":{\"color\":\"rgba(255,161,0,1)\",\"size\":[55,55],\"sizemode\":\"area\",\"line\":{\"color\":\"rgba(255,161,0,1)\"}},\"textfont\":{\"color\":\"rgba(255,161,0,1)\",\"size\":55},\"error_y\":{\"color\":\"rgba(255,161,0,1)\",\"width\":55},\"error_x\":{\"color\":\"rgba(255,161,0,1)\",\"width\":55},\"line\":{\"color\":\"rgba(255,161,0,1)\",\"width\":55},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\n## Word Frequency\nWord tokenization is applied before formal analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremove_reg <- \"&amp;|&lt;|&gt;\" #regular expression\nnewstops <- c('covid_19','covid-19','covid 19','coronavirus','covid19', '#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', '#StayHomeStaySafe', '#TestTraceIsolate') #hashtags that need to be removed\n\ncovid_19_vaccination <- covid_19_vaccination %>%  \n  mutate(text = str_remove_all(text, remove_reg)) %>%  #remove regular expression\n  unnest_tokens(word, text, token = 'tweets',strip_url = TRUE) %>% #work tokenizations\n  filter(!word %in% stop_words$word, #remove stopwords\n         !word %in% str_remove_all(stop_words$word, \"'\"),\n         !word %in% newstops, #remove those hashtags\n         str_detect(word, \"[a-z]\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing `to_lower = TRUE` with `token = 'tweets'` may not preserve URLs.\n```\n:::\n:::\n\n\n## Sentiment Scores: Anger, Anticipation, Disgust, Joy, Sadness, Surprise, Trust\n\nWe can not only analyse if the emotional tone of the tweets is positive or negative but also determine more nuanced emotions like anger, anticipation, disgust, joy, sadness, surprise or trust.\n\nIn the following graph primary emotions are differentiated.\n\n::: {.cell}\n\n```{.r .cell-code}\n#get words and their frequency\nfrequency_global <- covid_19_vaccination %>% count(word, sort=T) \n#get the top 10\nfrequency_global %>% top_n(10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n       word    n\n1     covid 9824\n2   vaccine 8279\n3   booster 1178\n4       flu 1122\n5  vaccines 1011\n6    pfizer  877\n7    people  813\n8   updated  677\n9      dose  635\n10    biden  625\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#anger,anticipation,disgust,joy,sadness,surprise,trust\ns <-get_nrc_sentiment(tweets)\ns_no_pos_neg <- select(s,anger,anticipation,disgust,joy,sadness,surprise,trust)\n\n#calculationg total score for each sentiment\ns_no_pos_neg <- data.frame(colSums(s_no_pos_neg[,]))\n\nnames(s_no_pos_neg) <- \"Score\"\ns_no_pos_neg <- cbind(\"sentiment\"=rownames(s_no_pos_neg), s_no_pos_neg)\nrownames(s_no_pos_neg) <- NULL\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#plotting the sentiments with scores\noptions(repr.plot.width=14, repr.plot.height=6)\nggplot(data=s_no_pos_neg,\n       aes(x=sentiment,y=Score))+\n  geom_bar(aes(fill=sentiment), stat = \"identity\", width=0.7)+\n  labs(caption = \"Status: 07/11/2022\")+\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(size = 10),\n        axis.text.y = element_text(size = 14),\n        axis.title.x = element_text(size = 10),\n        axis.title.y = element_text(size = 20),\n        plot.title = element_text(size = 20)) +\n  xlab(\"\")+\n  ylab(\"Score\")+\n  scale_colour_brewer(palette= \"Pastel1\")+\n  ggtitle(\"\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nPeople are showing a lot of trust for the covid vaccine on Twitter. The anticipation is high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_words <- tweets_bing %>%\n  # Count by word and sentiment\n  count(word, sentiment) %>%\n  group_by(sentiment) %>% #group ny sentiment\n  # Take the top 10 for each sentiment\n  top_n(10) %>%\n  ungroup() %>%\n  # Make word a factor in order of n\n  mutate(word = reorder(word, n))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in count(., word, sentiment): object 'tweets_bing' not found\n```\n:::\n\n```{.r .cell-code}\n#plot the result\nggplot(top_words, aes(word, n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = n, hjust=1), size = 3.5, color = \"black\") +\n  facet_wrap(~sentiment, scales = \"free\") +  \n  coord_flip() +\n  ggtitle(\"Most Common Positive and Negative words (Global)\") + \n  theme(plot.title = element_text(size = 14, face = \"bold\",hjust = 0.5))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in ggplot(top_words, aes(word, n, fill = sentiment)): object 'top_words' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncovid_19_vaccination %>%\n  # implement sentiment analysis using the \"nrc\" lexicon\n  inner_join(get_sentiments(\"nrc\")) %>%\n  # remove \"positive/negative\" sentiments\n  filter(!sentiment %in% c(\"positive\", \"negative\")) %>%\n  #get the frequencies of sentiments\n  count(sentiment,sort = T) %>% \n  #calculate the proportion\n  mutate(percent=100*n/sum(n)) %>%\n  select(sentiment, percent) %>%\n  #plot the result\n  chartJSRadar(showToolTipLabel = TRUE, main = \"NRC Radar\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n::: {.cell-output-display}\n```{=html}\n<canvas id=\"htmlwidget-5d7df13306c48923ff87\" class=\"chartJSRadar html-widget\" width=\"100%\" height=\"464px\"></canvas>\n<script type=\"application/json\" data-for=\"htmlwidget-5d7df13306c48923ff87\">{\"x\":{\"data\":{\"labels\":[\"fear\",\"trust\",\"anticipation\",\"sadness\",\"anger\",\"joy\",\"surprise\",\"disgust\"],\"datasets\":[{\"label\":\"percent\",\"data\":[24.1141183035714,18.7813895089286,13.2917131696429,12.6988002232143,10.6131417410714,7.4462890625,6.8115234375,6.24302455357143],\"backgroundColor\":\"rgba(255,0,0,0.2)\",\"borderColor\":\"rgba(255,0,0,0.8)\",\"pointBackgroundColor\":\"rgba(255,0,0,0.8)\",\"pointBorderColor\":\"#fff\",\"pointHoverBackgroundColor\":\"#fff\",\"pointHoverBorderColor\":\"rgba(255,0,0,0.8)\"}]},\"options\":{\"responsive\":true,\"title\":{\"display\":true,\"text\":\"NRC Radar\"},\"scale\":{\"ticks\":{\"min\":0},\"pointLabels\":{\"fontSize\":18}},\"tooltips\":{\"enabled\":true,\"mode\":\"label\"},\"legend\":{\"display\":true}}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\n## Sentiments Split by Word Frequency (Most Frequent Words)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Emotions Split by Words\noptions(repr.plot.width=14, repr.plot.height=8)\nunnest_tweets %>% \n  inner_join(nrc, \"word\") %>% \n  count(sentiment, word, sort=T) %>%\n  group_by(sentiment) %>% \n  arrange(desc(n)) %>% \n  slice(1:7) %>% \n  \n  # Plot:\n  ggplot(aes(x=reorder(word, n), y=n)) +\n  geom_col(aes(fill=sentiment), show.legend = F, width=0.7) +\n  facet_wrap(~sentiment, scales = \"free_y\", nrow = 2, ncol =5) +\n  coord_flip() +\n  labs(caption = \"Status: 13/11/2022\")+\n  theme(#plot.background = element_rect(fill = \"grey98\", color = \"grey20\"),\n        panel.background = element_rect(fill = \"grey98\"),\n        panel.grid.major = element_line(colour = \"grey87\"),\n        text = element_text(color = \"grey20\"),\n        plot.title = element_text(size = 9),\n        plot.subtitle = element_text(size = 9),\n        axis.title = element_text(size = 9),\n        axis.text = element_text(size = 9),\n        legend.box.background = element_rect(color = \"grey20\", \n                                             fill = \"grey98\", \n                                             size = 0.1),\n        legend.box.margin = margin(t = 3, r = 3, b = 3, l = 3),\n        legend.title = element_blank(),\n        legend.text = element_text(size = 9),\n        strip.text = element_text(size=9),\n        axis.text.x = element_blank()) + \n  labs(x=\"\", y=\"\", title=\"\") +\n  scale_colour_brewer(palette= \"BrBG\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\"Government\", \"pandemic\" and \"risk\" as reasons for fear and negative emotions. \"Vaccine\" as the most important reason for positive emotions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfav<-covid_19_vaccination %>%\n  #order the tweets descendingly by counts of favorites\n  arrange(desc(favoriteCount)) %>% \n  #select the text and count\n  dplyr:::select(text,favoriteCount) %>% \n  #get the top 5\n  head(5)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `dplyr:::select()`:\n! Can't subset columns that don't exist.\n✖ Column `text` doesn't exist.\n```\n:::\n\n```{.r .cell-code}\nkable(fav,format = \"html\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in kable(fav, format = \"html\"): object 'fav' not found\n```\n:::\n:::\n\n\n# Topic Modelling\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\nfindAssocs(dtm, \"health\",0.2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in findAssocs(dtm, \"health\", 0.2): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfindAssocs(dtm, \"pfizer\",0.2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in findAssocs(dtm, \"pfizer\", 0.2): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot = data.frame(words = names(freq), count = freq)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in data.frame(words = names(freq), count = freq): object 'freq' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nplot = subset(plot, plot$count > 400) #creating a subset of words having more than 100 frequency\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in plot$count: object of type 'closure' is not subsettable\n```\n:::\n\n```{.r .cell-code}\nstr(plot)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (x, y, ...)  \n```\n:::\n\n```{.r .cell-code}\nggplot(data = plot, aes(words, count)) + geom_col(width = 0.6, position = \"dodge\")+ ggtitle('Words used more than 400 times')+coord_flip() + theme(axis.text = element_text(size = 6))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `ggplot()`:\n! `data` cannot be a function.\nℹ Have you misspelled the `data` argument in `ggplot()`\n```\n:::\n:::\n\nLDA model with 5 topics selected\n\n::: {.cell}\n\n```{.r .cell-code}\nlda_5 = LDA(dtm, k = 5, method = 'Gibbs', \n          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, \n                         thin = 500, burnin = 4000, iter = 2000))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n#LDA model with 2 topics selected\nlda_2 = LDA(dtm, k = 2, method = 'Gibbs', \n          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, \n                         thin = 500, burnin = 4000, iter = 2000))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n\n```{.r .cell-code}\n#LDA model with 10 topics selected\nlda_10 = LDA(dtm, k = 10, method = 'Gibbs', \n          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, \n                         thin = 500, burnin = 4000, iter = 2000))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Top 10 terms or words under each topic\n\n\ntop10terms_5\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top10terms_5' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntop10terms_2\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top10terms_2' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntop10terms_10\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'top10terms_10' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlda.topics_5 = as.matrix(topics(lda_5))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'topics': object 'lda_5' not found\n```\n:::\n\n```{.r .cell-code}\nlda.topics_2 = as.matrix(topics(lda_2))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'topics': object 'lda_2' not found\n```\n:::\n\n```{.r .cell-code}\nlda.topics_10 = as.matrix(topics(lda_10))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'topics': object 'lda_10' not found\n```\n:::\n\n```{.r .cell-code}\nsummary(as.factor(lda.topics_5[,1]))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is.factor(x): object 'lda.topics_5' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndoc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'rowSums': cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\ndtm <- DocumentTermMatrix(corpus[doc.lengths > 0])\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in TermDocumentMatrix(x, control): object 'doc.lengths' not found\n```\n:::\n\n```{.r .cell-code}\ndtm <- TermDocumentMatrix(corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\n# Now for some topics\nSEED = sample(1:1000000, 1)  # Pick a random seed for replication\nk = 10  # Let's start with 10 topics\n\n# This might take a minute!\nmodels <- list(\n    CTM       = CTM(dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))),\n    VEM       = LDA(dtm, k = k, control = list(seed = SEED)),\n    VEM_Fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),\n    Gibbs     = LDA(dtm, k = k, method = \"Gibbs\", control = list(seed = SEED, burnin = 1000,\n                                                                 thin = 100,    iter = 1000))\n)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in is(x, \"DocumentTermMatrix\"): object 'dtm' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# There you have it. Models now holds 4 topics. See the topicmodels API documentation for details\n\n# Top 10 terms of each topic for each model\n# Do you see any themes you can label to these \"topics\" (lists of words)?\ndtm <- TermDocumentMatrix(corpus)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in as.character(x$content): cannot coerce type 'closure' to vector of type 'character'\n```\n:::\n\n```{.r .cell-code}\nlapply(models, terms, 10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lapply(models, terms, 10): object 'models' not found\n```\n:::\n\n```{.r .cell-code}\n# matrix of tweet assignments to predominate topic on that tweet\n# for each of the models, in case you wanted to categorize them\nassignments <- sapply(models, topics)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lapply(X = X, FUN = FUN, ...): object 'models' not found\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Tokenizing character vector file 'tweets'.\ntoken = data.frame(text=tweets, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)\n\n#Matching sentiment words from the 'NRC' sentiment lexicon\nsenti = inner_join(token, get_sentiments(\"nrc\")) %>%\n  count(sentiment)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n```{.r .cell-code}\nsenti$percent = (senti$n/sum(senti$n))*100\n\n#Plotting the sentiment summary \nggplot(senti, aes(sentiment, percent)) +   \n        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+ \n        ggtitle(\"Sentiment analysis based on lexicon: 'NRC'\")+\n  coord_flip() +\n        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),\n              axis.text=element_text(size=16),\n              axis.title=element_text(size=14,face=\"bold\"))\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n### Additional analysis: Sentiment analysis on 'booster' topic\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_booster = corpus(tweets)\ncorpus_booster = (corpus_booster = subset(corpus_booster, grepl('booster', texts(corpus_booster))))\nwriteLines(as.character(corpus_booster[[150]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe #LivCo Department of Health will be offering COVID-19 booster shots and flu vaccine at 2 Murray Hill Drive in M… https://t.co/jip24oyAcJ...\n```\n:::\n\n```{.r .cell-code}\ntoken_booster = data.frame(text=corpus_booster, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)\n\n#Matching sentiment words from the 'NRC' sentiment lexicon\nlibrary(dplyr)\nsenti_booster = inner_join(token_booster, get_sentiments(\"nrc\")) %>%\n  count(sentiment)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n:::\n\nPlotting the sentiment summary :\n\n::: {.cell}\n\n```{.r .cell-code}\nsenti_booster$percent = (senti_booster$n/sum(senti_booster$n))*100\n\n#Plotting the sentiment summary \nlibrary(ggplot2)\nggplot(senti_booster, aes(sentiment, percent)) +   \n        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+ \n        ggtitle(\"Sentiment analysis summary on Booster lexicon: 'NRC'\")+\n  coord_flip() +\n        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),\n              axis.text=element_text(size=16),\n              axis.title=element_text(size=14,face=\"bold\"))\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-52-1.png){width=672}\n:::\n:::\n\n### Additional analysis: Sentiment analysis on 'pfizer' topic\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_pfizer = corpus(tweets)\ncorpus_pfizer = (corpus_pfizer = subset(corpus_pfizer, grepl('pfizer', texts(corpus_pfizer))))\nwriteLines(as.character(corpus_pfizer[[34]]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n@mareefeb @kach022 @Tabithatribex4 @PassengerShame @American_proud7 @GABOY1331 @pfizer Is isnt the vaccine… https://t.co/ozyCIzni01...\n```\n:::\n\n```{.r .cell-code}\ntoken_pfizer = data.frame(text=corpus_pfizer, stringsAsFactors = FALSE) %>% unnest_tokens(word, text)\n\n#Matching sentiment words from the 'NRC' sentiment lexicon\nlibrary(dplyr)\nsenti_pfizer = inner_join(token_pfizer, get_sentiments(\"nrc\")) %>%\n  count(sentiment)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n:::\n\nPlotting the sentiment summary\n\n::: {.cell}\n\n```{.r .cell-code}\nsenti_pfizer$percent = (senti_pfizer$n/sum(senti_pfizer$n))*100\n\n#Plotting the sentiment summary \nlibrary(ggplot2)\nggplot(senti_pfizer, aes(sentiment, percent)) +   \n        geom_bar(aes(fill = sentiment), position = 'dodge', stat = 'identity')+ \n        ggtitle(\"Sentiment analysis on Pfizer based on lexicon: 'NRC'\")+\n  coord_flip() +\n        theme(legend.position = 'none', plot.title = element_text(size=18, face = 'bold'),\n              axis.text=element_text(size=16),\n              axis.title=element_text(size=14,face=\"bold\"))\n```\n\n::: {.cell-output-display}\n![](Final-Project_TextAsData_Kaushika-Potluri_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\nOver all we can see that the emotion for the Covid-19 Vaccine is positive.\n\n# Conclusion\nI conclude by saying Public COVID-19 vaccine-related discussion on Twitter was largely driven by major events about COVID-19 vaccines and mirrored the active news topics in mainstream media. The discussion also demonstrated a global perspective. The increasingly positive sentiment around COVID-19 vaccines and the dominant emotion of trust shown in the social media discussion may imply higher acceptance of COVID-19 vaccines compared with previous vaccines.\n#References :\n\n[1] Negative COVID-19 Vaccine Information on Twitter: Content\nAnalysis by Yiannakoulias N, Darlington JC, Slavik CE, Benjamin G.\n\n[2]COVID-19 Vaccine-Related Discussion on Twitter: Topic\nModeling and Sentiment Analysis by Joanne Chen Lyu, Eileen Le\n\n[3] Public Opinion and Sentiment Before and at the Beginning of\nCOVID-19 Vaccinations in Japan: Twitter Analysis. by Niu Q, Liu J,Kato M, Shinohara Y, Matsumura N, Aoyama T, Nagai-Tanima M.",
    "supporting": [
      "Final-Project_TextAsData_Kaushika-Potluri_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../site_libs/plotly-binding-4.10.1/plotly.js\"></script>\n<script src=\"../site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"../site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n<link href=\"../site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n<script src=\"../site_libs/chart.js-2.4.0/./dist/Chart.min.js\"></script>\n<script src=\"../site_libs/chartJSRadar-binding-0.3.1/chartJSRadar.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}